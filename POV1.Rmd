---
title: "Point of View & Perspective Taking"
author: "Kathryn Denning"
date: "March 23, 2020"
output: 
  html_document:
    code_folding: "hide"
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

# Introduction to study

This study looked at whether the point of view people responded in when writing about the day in the life of a target individual (e.g., a perspective taking manipulation) impacted the extent they projected (Analysis 1) and stereotyped (Analysis 2) with the target individual. We also checked to see if the point of view condition predicted self-reported perspective taking by participants (Analysis 3). Due to an issue with singularity in our MLM in Analysis 1, we also analyzed difference scores in projection as the outcome variable, instead of item level scores for projection (see difference score results in Analysis 4). 

Variable names that might be confusing:

* **pt_condition** : What perpsective taking condition participants were randomly assigned. Helmert contrasts were applied to this variable. 
    + 1 = 1st person
    + 2 = 3rd person
    + 3 = Control
    + 4 = Objective
* **targ** : Participants' responses for the target on the BFI.
* **self_c** : Participants' responses for themselves on the BFI. This variable was mean-centered.
* **pt_mancheck** : Participants' self-reported responses on how much they took the perspective of the target. This was coded as follows, but reverse coded for the plot in Analysis 3 to make it more interpretable (that higher scores meant more PT). 
    + 1 = A great deal
    + 2 = A lot
    + 3 = A moderate amount
    + 4 = A little
    + 5 = None at all
* **diff_score** : The difference in the avarage scores of participants' on the BFI for themselves minus the BFI for the target. These scores were not absolute valued.

# Alpha for the ageism scale so it can be collapse into one composite score per participant

```{r setup data import and cleaning, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
# Cleaning
## import data

#install.packages("rio")
#install.packages("here")
#install.packages("tidyverse")
#install.packages("magrittr")
#install.packages("janitor")
#install.packages("lme4")
#install.packages("psy")
#install.packages("irr")
#install.packages("emmeans")
#install.packages("sjPlot")
#install.packages("effects")
library(rio)
library(here)
library(tidyverse)
library(magrittr)
library(janitor)
library(lme4)
library(psy)
library(irr)
library(psych)
library(sjPlot)
library(emmeans)
library(effects)
library(readxl)

# Setting global chunk options
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# Data after manually removing participants who did not correctly complete the PT manipulation
pov <- read_excel(here("Denning_POV.xlsx")) 

# Reverse coding
pov_recode <- pov %>% 
     mutate_at(c("bfi_self_quiet", "bfi_self_rude","bfi_self_disorg",
                 "bfi_self_taskdiff", "bfi_self_stable", "bfi_self_abstract",
                 "bfi_self_risk", "bfi_self_use", "bfi_targ_quiet", 
                 "bfi_targ_rude", "bfi_targ_disorg", "bfi_targ_taskdiff",
                 "bfi_targ_stable", "bfi_targ_abstract", "bfi_targ_risk",
                 "bfi_targ_use"), 
               list(~recode(., `1`= 5, `2`= 4, `3` = 3, `4` = 2, `5` = 1)))

# Looking at scale reliability for ageism
pov_recode %>% 
  select(ageism_1:ageism_10) %>% 
  cronbach()
#Looks good enough to collapse as one score

# Cleaning

pov_clean <- pov_recode %>% 
  # Getting data into long-format with variables for BFI for Self, Targ, PT_Condition, and Ageism
    pivot_longer(c(bfi_self_quiet:bfi_self_rules, 
                   bfi_targ_quiet:bfi_targ_rules),
           names_sep = "_",
           names_to = c("drop1", "bfi_type", "bfi_qtype")) %>% 
  pivot_wider(names_from = bfi_type, values_from = value) %>% 
  pivot_longer(c(pt_control:pt_obj),
               names_sep = "_",
               names_to = c("drop2", "pt_condition")) %>% 
  filter(value != "NA") %>% 
  select(-c(drop1, drop2, value)) %>% 
  pivot_longer(c(ageism_1:ageism_10),
               names_sep = "_",
               names_to = c("drop3", "ageism_itemnum")) %>%
  # Change label of value to ageism, make age numeric, make gender and race categorical with labels, make pt_condition a factor
  # Reverse coding the manipulation check to make it more intuitive to interpret
  mutate(ageism = value,
         age = as.numeric(age),
         race = as.factor(recode(race,
                        `1` = "American Indian/Alaska Native",
                        `2` = "Asian",
                        `3` = "Black",
                        `5` = "Latinx",
                        `6` = "Middle Eastern",
                        `4` = "Pacific Islander",
                        `7` = "White",
                        `8` = "Other",
                        `9` = "Prefer not to answer")),
         gender = as.factor(recode(gender,
                         `1` = "Female",
                         `2` = "Male",
                         `3` = "Non-binary",
                         `4` = "Other",
                         `5` = "Prefer not to say")),
         pt_condition = as.factor(pt_condition),
         self_c = self - mean(self, na.rm = TRUE)) %>% 
    # getting a composite score for ageism
  group_by(subid) %>% 
  mutate(ageism_ave = mean(ageism)) %>% 
  select(-c(drop3, value, gender_txt, race_txt, ageism, ageism_itemnum)) %>% 
    unique() %>% 
  na.omit()
  
# check man check before analysis
# higher scores on rupp = stereotyping
```

# Demographics
## Sample Size

```{r sample size}
pov_clean %>% 
  select(subid) %>% 
  unique() %>% 
  na.omit() %>% 
  nrow()
```

## Participants per condition

```{r participants per condition}
pov_clean %>% 
  select(subid, pt_condition) %>% 
  unique() %>% 
  na.omit() %>% 
  group_by(pt_condition) %>% 
  count()
```

## Age

### Mean

```{r mean age}
mean(pov_clean$age)
```

### SD

```{r sd age}
sd(pov_clean$age)
```

## Gender

```{r gender}
pov_clean %>% 
  select(subid, gender) %>% 
  unique() %>% 
  na.omit() %>% 
  group_by(gender) %>% 
  count()
```

## Race

```{r race}
pov_clean %>% 
  select(subid, race) %>% 
  unique() %>% 
  na.omit() %>% 
  group_by(race) %>% 
  count() %>% 
  mutate(percent = n/439*100)
```

# Descriptives
## Histogram of distributions
### BFI for Self

```{r hist self}
hist_self <- pov_clean %>% 
  select(subid, self) %>% 
  unique()
hist(hist_self$self)
```

### BFI for Target

```{r hist targ}
hist_targ <- pov_clean %>% 
  select(subid, targ) %>% 
  unique()
hist(hist_targ$targ)
```

## Histogram for PT manipulation check

```{r hist pt check}
hist_ptcheck <- pov_clean %>% 
  select(subid, pt_mancheck) %>% 
  mutate(pt_mancheck = as.numeric(pt_mancheck)) %>% 
  unique()
hist(hist_ptcheck$pt_mancheck)
```

1 = A great deal
2 = A lot
3 = A moderate amount
4 = A little
5 = None at all

So, people are  skewed toward reporting they were perspective taking, which generally they should be based on the instructions

## Histogram for ageism scale

```{r hist ageism}
hist_ageism <- pov_clean %>% 
  select(subid, ageism_ave) %>% 
  unique()
hist(hist_ageism$ageism_ave)
```

## Checking level 1 heteroscedasticity
```{r hetero}
hetero <- lm(targ ~ self, data = pov_clean)
par(mfrow=c(2,2)) # init 4 charts in 1 panel
plot(hetero)
```

Flat line in residuals vs fitted looks good!

## Mean and SD for BFI by PT condition

### BFI for Self

```{r BFI self}
pov_clean %>% 
  select(subid, pt_condition, self) %>% 
  na.omit() %>% 
  group_by(pt_condition) %>% 
  summarize(mean = mean(self),
            sd = sd(self))
```

### BFI for Target

```{r BFI target}
pov_clean %>% 
  select(subid, pt_condition, targ) %>% 
  na.omit() %>% 
  group_by(pt_condition) %>% 
  summarize(mean = mean(targ),
            sd = sd(targ))
```

## Mean and SD for ageism by condition

```{r mean ageism}
ageism_descrip <- pov_clean %>%
  select(subid, pt_condition, ageism_ave) %>% 
  na.omit() %>% 
  group_by(pt_condition) %>% 
  summarize(mean = mean(ageism_ave),
            sd = sd(ageism_ave),
            se = sd(ageism_ave)/sqrt(length(ageism_ave)))
ageism_descrip
```

## Overall descriptives

```{r overall descriptives}
pov_clean %>% 
  select(age, gender, race, pt_mancheck, self, targ, 
         pt_condition, self_c, ageism_ave) %>% 
  describe()
```

Only variable that is skewed is age due to an outlier of an older participant.

# Analysis 1: MLM model for projection

## Complex random effects model we predicted

```{r projection mlm}
proj_model <- pov_clean %>% 
  select(targ, self_c, pt_condition, subid) %>% 
  unique() %>% 
  na.omit()

contrasts(proj_model$pt_condition) <- "contr.helmert"
contrasts(proj_model$pt_condition)

projection <- lmer(targ ~ self_c*pt_condition + (self_c|subid), data = proj_model)
summary(projection)
tab_model(projection)
```

## Fixed effects model

```{r fixed effects}
projection_fixed <- lmer(targ ~ self_c*pt_condition + (1|subid), data = proj_model)
summary(projection_fixed)
tab_model(projection_fixed)
```

## Model with just L1 predictor of self, no random effects

```{r fixed effects, just L1}
projection_fixed_l1 <- lmer(targ ~ self_c + (1|subid), data = proj_model)
summary(projection_fixed_l1)
```

## Model with simplest random effects structure

```{r projection simple}
projection_simplest <- lmer(targ ~ 1 + (1|subid), data = proj_model)
summary(projection_simplest)
```

Model converged but has singularity issues, meaning the random effect for the item level is pretty much zero (which it is, you can see this under "Random Effects" in the "summary(projection)" output, where our item level data is labeled "self_c." I initially thought this might be because the random structure of the model was too complex for the data, but, even after running a model with the simplest random structure possible, I was still getting a singularity issue. Previously I have gotten this due to an error in contrasts, but I checked and that is not the case here. 

After some more researching, I've basically concluded we are getting a singularity warning because there is so little variance in the item-level of our model. Typically, a singularity issue means you would not want to report that model, regardless, nothing but the main effect of self scores on BFI predicting target scores on BFI was significant anyways. I'm not super suprised, as the scores for the mean and SD for BFI for both self and target per condition are very similiar, so it really looks like there isn't much going on in the projection data.

# Analysis 2: Anova for stereotyping

```{r stereo anova}
stereo_model <- pov_clean %>% 
  select(ageism_ave, pt_condition, subid) %>% 
  unique() %>% 
  na.omit()

contrasts(stereo_model$pt_condition) <- "contr.helmert"
contrasts(stereo_model$pt_condition) 

stereo <- lm(ageism_ave ~ pt_condition, data = stereo_model)
summary(stereo)
```

Only a marginal effect of 1st and 3rd person versus control.

## Plot for trending main effect

```{r stereo plot}
ggplot(ageism_descrip, aes(pt_condition, mean, fill = pt_condition)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  scale_fill_viridis_d() +
  theme(legend.position = "none") +
  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = .2,
                position = position_dodge(.9)) +
  labs(title = "Stereotyping predicted by perspective taking condition",
       subtitle = "Higher stereotyping scores indicate more stereotyping",
       x = "Perspective taking condition",
       y = "Stereotyping score")
```

# Analysis 3: Anova model predicting manipulation check by condition

```{r pt mancheck}
pt_check_model <- pov_clean %>% 
  select(pt_mancheck, pt_condition, subid) %>% 
  unique() %>% 
  na.omit()

contrasts(pt_check_model$pt_condition) <- "contr.helmert"
contrasts(pt_check_model$pt_condition) 

ptcheck_model <- lm(pt_mancheck ~ pt_condition, data = pt_check_model)
summary(ptcheck_model)
```

## Plot of condition predicting manipulation check

```{r pt man descrip and plot}
pt_check_data <- pt_check_model %>% 
   mutate(pt_mancheck_recode = recode(pt_mancheck,
                                 `1` = "5",
                                 `2` = "4",
                                 `3` = "3",
                                 `4` = "2",
                                 `5` = "1")) %>% 
  mutate(pt_manceck_recode_num = as.numeric(pt_mancheck_recode)) %>% 
  group_by(pt_condition) %>% 
  summarize(mean = mean(pt_manceck_recode_num),
            sd = sd(pt_manceck_recode_num),
            se = sd(pt_manceck_recode_num)/sqrt(length(pt_manceck_recode_num)))
  
ggplot(pt_check_data, aes(pt_condition, mean, fill = pt_condition)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  scale_fill_viridis_d() +
  theme(legend.position = "none") +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = .2,
                position = position_dodge(.9)) +
  labs(title = "Self-reported perspective taking by perspective taking condition",
       subtitle = "Higher stereotyping scores indicate more self-reported perspective taking",
       x = "Perspective taking condition",
       y = "Self-reported perspective taking")
```

The contrast that was significant was objective vs the other three (though the other two were marginal), which can be seen in this plot. Not surprising that we find less self-reported PT in the objective condition but not in the other three, though interesting we didn't find any differences between objective and the other conditions in our outcome measures.

# Analysis 4: Model predicting difference scores for projection by condition
## Preview of the difference score data - it can have both positives and negatives, did not use absolute values

```{r pt cond diff scores data}
diff_score_data <- pov_clean %>% 
  select(subid, pt_condition, targ, self) %>% 
  group_by(subid) %>% 
  mutate(targ_ave = mean(targ),
         self_ave = mean(self)) %>% 
  select(-c(targ, self)) %>% 
  unique() %>% 
  na.omit() %>% 
  mutate(diff_score = self_ave-targ_ave)
diff_score_data
```

## Results of difference score anova

```{r diff score model}
diff_score_data %>% 
  group_by(pt_condition) %>% 
  summarize(mean = mean(diff_score),
         sd = sd(diff_score))

contrasts(diff_score_data$pt_condition) <- "contr.helmert"
contrasts(diff_score_data$pt_condition)

diff_score_model <- lm(diff_score ~ pt_condition, data = diff_score_data)
summary(diff_score_model)
```

Nothing is significant.
